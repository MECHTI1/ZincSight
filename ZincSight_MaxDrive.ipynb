{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2227a5adfad709",
   "metadata": {
    "id": "2c2227a5adfad709"
   },
   "source": [
    "# **ZincSight**: Interpretable prediction of zinc ion locations in proteins\n",
    "\n",
    "‚ö†Ô∏è **Important Instructions:**\n",
    "\n",
    "- If ZincSight crashes, please reset the runtime manually:\n",
    "  Go to `Runtime` ‚Üí `Disconnect and delete runtime`, then refresh the page and try again.\n",
    "  <p></p>\n",
    "- If the issue continues, contact **mechtinger1@mail.tau.ac.il** and include the **protein IDs or structures** you used for reproduction and debugging.\n",
    "  <p></p>\n",
    "- For maximum speed, change the hardware accelerator: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí select `TPU`.\n",
    "  <p></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e70ba43144b2857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e70ba43144b2857",
    "outputId": "156427d1-99e0-47e2-d687-77bdf38fb7f5"
   },
   "source": [
    "#@title üß¨ ZincSight: Structure Input & Configuration {display-mode: \"form\"}\n",
    "\n",
    "#@markdown **Enter your protein(s)** in the field below or upload **PDB** or **mmCIF** structures.\n",
    "#@markdown - Query IDs: **PDB** (ASU or biological assembly), **AlphaFoldDB** (UniProt or Model ID), **ESM Metagenomic Atlas**, or **TED Domains**.\n",
    "#@markdown - To upload your own structure files (including `.tar.gz`), check the appropriate box below.\n",
    "#@markdown - **Note:** All input folders will be flattened (all files moved to the root query folder).\n",
    "#@markdown - **Drive import auto-detect:** folder/archive => structures; .txt/.csv/.tsv => ID list; .pdb/.cif/.ent(.gz) => structure file.\n",
    "\n",
    "# --- UI CONTROLS ---\n",
    "\n",
    "#@markdown ### üîë 1. Enter Identifiers\n",
    "structure_ids = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### üìÇ 2. Import Local Files\n",
    "upload_structures = False #@param {type:\"boolean\"}\n",
    "upload_tar_gz = False #@param {type:\"boolean\"}\n",
    "upload_id_list_txt = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ‚òÅÔ∏è 3. Import from Google Drive\n",
    "import_from_google_drive = True # @param {\"type\":\"boolean\"}\n",
    "drive_path = \"\" #@param {type:\"string\"}\n",
    "#@markdown *Path relative to 'My Drive'. Can be a file, folder, or .tar.gz archive.*\n",
    "#@markdown *Drive import auto-detect:*\n",
    "#@markdown - **Folder** ‚Üí **structure files** (copied + flattened into `/content/query_structures`)\n",
    "#@markdown - **.tar.gz / .tgz** ‚Üí **structures archive** (extracted + flattened)\n",
    "#@markdown - **.pdb / .cif / .pdb.gz / .cif.gz** ‚Üí **single structure file** (copied)\n",
    "#@markdown - **.txt/ .csv / .tsv** ‚Üí **ID list** (comma/whitespace-separated)\n",
    "#@markdown ---\n",
    "#@markdown ### ‚öôÔ∏è 4. Settings\n",
    "initialize_fresh_query = True #@param {type:\"boolean\"}\n",
    "include_histidine_rotamers = False #@param {type: \"boolean\"}\n",
    "create_pymol_sessions = False #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### üìÅ 5. Output folder name\n",
    "name_of_output_folder = \"\" #@param {type:\"string\"}\n",
    "#@markdown *If relative, it will be created under MyDrive. If absolute (e.g., /content/results), it will be created there.*\n",
    "\n",
    "# --- IMPLEMENTATION ---\n",
    "\n",
    "import re, shutil, tarfile, os\n",
    "from pathlib import Path\n",
    "from google.colab import files, drive\n",
    "\n",
    "QUERY_DIR = Path(\"/content/query_structures\")\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Wipes and recreates the query directory if requested.\"\"\"\n",
    "    if initialize_fresh_query and QUERY_DIR.exists():\n",
    "        print(\"üßπ Initializing query: clearing previous files.\")\n",
    "        shutil.rmtree(QUERY_DIR)\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def flatten_directory():\n",
    "    \"\"\"Moves all files from subdirectories to the root QUERY_DIR and removes empty dirs.\"\"\"\n",
    "    # Move files up\n",
    "    for p in list(QUERY_DIR.rglob(\"*\")):\n",
    "        if p.is_file() and p.parent != QUERY_DIR:\n",
    "            dest = QUERY_DIR / p.name\n",
    "            # Overwrite if exists (simple behavior)\n",
    "            try:\n",
    "                if dest.exists():\n",
    "                    dest.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "            shutil.move(str(p), str(dest))\n",
    "\n",
    "    # Remove empty directories\n",
    "    for p in sorted(list(QUERY_DIR.rglob(\"*\")), reverse=True):\n",
    "        if p.is_dir() and p != QUERY_DIR:\n",
    "            try:\n",
    "                p.rmdir()\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "def extract_archive(file_path: Path):\n",
    "    \"\"\"Extracts tar.gz/tgz and flattens hierarchy.\"\"\"\n",
    "    print(f\"üì¶ Extracting: {file_path.name}...\")\n",
    "    try:\n",
    "        with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=QUERY_DIR)\n",
    "        flatten_directory()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction error: {e}\")\n",
    "\n",
    "def parse_ids(text: str):\n",
    "    \"\"\"Tokenizes string input into a list of IDs (comma/whitespace separated).\"\"\"\n",
    "    return [t for t in re.split(r\"[\\s,]+\", text.strip()) if t]\n",
    "\n",
    "def ensure_output_dir(path_str: str) -> Path:\n",
    "    \"\"\"\n",
    "    Creates output directory if it doesn't exist.\n",
    "    - If absolute (starts with /), it is used as-is.\n",
    "    - If relative, it is created under /content/drive/MyDrive/<path_str>.\n",
    "    \"\"\"\n",
    "    if not path_str or not path_str.strip():\n",
    "        raise ValueError(\"name_of_output_folder is empty\")\n",
    "\n",
    "    p = Path(path_str.strip())\n",
    "    if not p.is_absolute():\n",
    "        p = Path(\"/content/drive/MyDrive\") / p\n",
    "\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def handle_drive_import(rel_path: str, final_ids: list):\n",
    "    \"\"\"\n",
    "    Auto-detects Drive content:\n",
    "      - folder => copy all files (structures) + flatten\n",
    "      - .tar.gz/.tgz => extract (structures) + flatten\n",
    "      - .txt/.csv/.tsv => parse IDs into final_ids\n",
    "      - .pdb/.cif/.ent(.gz) => copy as structure\n",
    "      - otherwise => copy and warn\n",
    "    \"\"\"\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    src = Path(\"/content/drive/MyDrive\") / rel_path.lstrip(\"/\")\n",
    "\n",
    "    if not src.exists():\n",
    "        print(f\"‚ùå Not found on Drive: {src}\")\n",
    "        return final_ids\n",
    "\n",
    "    # Archives of structures\n",
    "    if src.name.endswith((\".tar.gz\", \".tgz\")):\n",
    "        extract_archive(src)\n",
    "        return final_ids\n",
    "\n",
    "    # Folder of structures\n",
    "    if src.is_dir():\n",
    "        print(f\"üìÇ Importing structure files from Drive folder: {src} (Flattening...)\")\n",
    "        count = 0\n",
    "        for f in src.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                shutil.copy2(f, QUERY_DIR)\n",
    "                count += 1\n",
    "        print(f\"   -> Copied {count} files.\")\n",
    "        flatten_directory()\n",
    "        return final_ids\n",
    "\n",
    "    # Single file\n",
    "    suffix = src.suffix.lower()\n",
    "    name_lower = src.name.lower()\n",
    "\n",
    "    # ID list files\n",
    "    if suffix in {\".txt\", \".csv\", \".tsv\"}:\n",
    "        text = src.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "        new_ids = parse_ids(text)\n",
    "        final_ids.extend(new_ids)\n",
    "        print(f\"üßæ Parsed IDs from Drive file: {src.name} -> +{len(new_ids)} IDs\")\n",
    "        return final_ids\n",
    "\n",
    "    # Structure files (including .pdb.gz etc.)\n",
    "    if suffix in {\".pdb\", \".cif\", \".ent\"} or name_lower.endswith((\".pdb.gz\", \".cif.gz\", \".ent.gz\")):\n",
    "        shutil.copy2(src, QUERY_DIR)\n",
    "        print(f\"üß¨ Imported structure file from Drive: {src.name}\")\n",
    "        return final_ids\n",
    "\n",
    "    # Unknown file type\n",
    "    shutil.copy2(src, QUERY_DIR)\n",
    "    print(f\"‚ö†Ô∏è Drive file type not recognized as ID list or structure: {src.name} (copied anyway).\")\n",
    "    return final_ids\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "setup_environment()\n",
    "final_ids = parse_ids(structure_ids)\n",
    "\n",
    "# Local Uploads: ID list\n",
    "if upload_id_list_txt:\n",
    "    print(\"üì§ Upload .txt ID list:\")\n",
    "    for name, content in files.upload().items():\n",
    "        final_ids.extend(parse_ids(content.decode(\"utf-8\", errors=\"replace\")))\n",
    "\n",
    "# Local Uploads: structures\n",
    "if upload_structures:\n",
    "    print(\"üì§ Upload PDB/mmCIF structures:\")\n",
    "    os.chdir(QUERY_DIR)\n",
    "    _ = files.upload()\n",
    "    os.chdir(\"/content\")\n",
    "\n",
    "# Local Uploads: archive of structures\n",
    "if upload_tar_gz:\n",
    "    print(\"üì§ Upload .tar.gz archive:\")\n",
    "    for name, content in files.upload().items():\n",
    "        tmp = Path(\"/content\") / name\n",
    "        tmp.write_bytes(content)\n",
    "        extract_archive(tmp)\n",
    "        tmp.unlink(missing_ok=True)\n",
    "\n",
    "# Drive Import (auto-detect)\n",
    "# Also mount drive if output folder is relative (MyDrive)\n",
    "need_drive = (import_from_google_drive and bool(drive_path)) or (not name_of_output_folder.is_absolute())\n",
    "if need_drive:\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "if import_from_google_drive and drive_path:\n",
    "    final_ids = handle_drive_import(drive_path, final_ids)\n",
    "\n",
    "# Ensure output folder exists\n",
    "OUT_DIR = ensure_output_dir(name_of_output_folder)\n",
    "print(f\"üìÅ Output folder ready: {OUT_DIR}\")\n",
    "\n",
    "# Force flatten one last time to be safe\n",
    "flatten_directory()\n",
    "\n",
    "# --- RESULTS SUMMARY ---\n",
    "\n",
    "# de-duplicate IDs while preserving order\n",
    "unique_ids = list(dict.fromkeys(final_ids))\n",
    "structure_ids_for_download = \",\".join(unique_ids)\n",
    "\n",
    "# Identify structure files robustly (handle .pdb.gz etc.)\n",
    "found_structures = [\n",
    "    f for f in QUERY_DIR.glob(\"*\")\n",
    "    if f.is_file() and f.name.lower().endswith((\".pdb\", \".cif\", \".ent\", \".pdb.gz\", \".cif.gz\", \".ent.gz\"))\n",
    "]\n",
    "\n",
    "# Optional: ID-list-like files present in the query folder (informational)\n",
    "id_list_files_in_query = [\n",
    "    f for f in QUERY_DIR.glob(\"*\")\n",
    "    if f.is_file() and f.name.lower().endswith((\".txt\", \".csv\", \".tsv\"))\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*50)\n",
    "print(\"üöÄ **ZincSight: Input Ready (FLATTENED)**\")\n",
    "print(f\"‚Ä¢ Identifiers to fetch: {len(unique_ids)}\")\n",
    "print(f\"‚Ä¢ Structure files detected: {len(found_structures)}\")\n",
    "if id_list_files_in_query:\n",
    "    print(f\"‚Ä¢ Note: ID-list-like files in query folder: {len(id_list_files_in_query)} (not treated as structures)\")\n",
    "print(\"‚Ä¢ Folder structure: All files moved to root ‚úÖ\")\n",
    "print(f\"‚Ä¢ Output folder: {OUT_DIR}\")\n",
    "\n",
    "if found_structures:\n",
    "    print(f\"‚Ä¢ Sample structure file: {found_structures[0].name}\")\n",
    "\n",
    "if not unique_ids and not found_structures:\n",
    "    print(\"\\n‚ö†Ô∏è **Warning:** No data found. Check your inputs.\")\n",
    "print(\"‚îÄ\"*50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad67a174c4150f7",
   "metadata": {
    "collapsed": true,
    "id": "8ad67a174c4150f7"
   },
   "outputs": [],
   "source": [
    "#@title Execute ZincSight (Auto-Batching 50k local files + Chunked ID Downloads + Skip-If-Already-Exists + Resume Log) {display-mode: \"form\"}\n",
    "from IPython.utils.capture import capture_output\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "import platform\n",
    "import re\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------\n",
    "# 0) USER PARAMS (expected to exist from earlier form cell)\n",
    "# -------------------------\n",
    "# include_histidine_rotamers : bool\n",
    "# create_pymol_sessions      : bool\n",
    "# name_of_output_folder      : str\n",
    "# structure_ids_for_download : str\n",
    "\n",
    "# -------------------------\n",
    "# 1) SETUP & DEPENDENCIES\n",
    "# -------------------------\n",
    "SETUP_MARKER = \"/content/ENV_SETUP.marker\"\n",
    "if not os.path.exists(SETUP_MARKER):\n",
    "    print(\"üîß Installing dependencies...\")\n",
    "    with capture_output():\n",
    "        if not os.path.exists(\"/content/ZincSight\"):\n",
    "            !git clone https://github.com/MECHTI1/ZincSight.git\n",
    "        %cd /content/ZincSight\n",
    "        !pip install -r requirements.txt\n",
    "        sys.path.append(\"/content/ZincSight\")\n",
    "    open(SETUP_MARKER, \"w\").close()\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "sys.path.insert(0, \"/content/ZincSight\")\n",
    "from main_execute import execute_zincsight\n",
    "\n",
    "# -------------------------\n",
    "# 2) CONFIGURATION\n",
    "# -------------------------\n",
    "ROOT_QUERY = Path(\"/content/query_structures\")\n",
    "OUTPUT_DIR = Path(\"/content/output\")\n",
    "DRIVE_DEST = Path(OUT_DIR)\n",
    "LOG_FILE = DRIVE_DEST / \"zincsight_batch_log.txt\"\n",
    "\n",
    "BATCH_SIZE = 50000          # MAX LOCAL FILES PER BATCH\n",
    "DOWNLOAD_ID_BATCH = 50000    # MAX IDs PER DOWNLOAD BATCH\n",
    "\n",
    "ROOT_QUERY.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DRIVE_DEST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hardware check\n",
    "physical_cores = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "def parse_structure_ids(raw: str):\n",
    "    \"\"\"\n",
    "    Accept IDs separated by commas / spaces / newlines.\n",
    "    Returns a clean list (order preserved).\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return []\n",
    "    parts = re.split(r\"[,\\s]+\", raw.strip())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def existing_structure_keys(root: Path):\n",
    "    \"\"\"\n",
    "    Build a set of 'keys' for existing structure files already present under root.\n",
    "    Key = filename without common extensions (pdb/cif/mmcif/ent + optional gz).\n",
    "    Example: AF-A0A...-F1-model_v4.pdb.gz -> AF-A0A...-F1-model_v4\n",
    "    \"\"\"\n",
    "    keys = set()\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        name = p.name\n",
    "\n",
    "        # strip gzip if present\n",
    "        if name.endswith(\".gz\"):\n",
    "            name = name[:-3]\n",
    "\n",
    "        # strip common structure extensions\n",
    "        for ext in (\".pdb\", \".cif\", \".mmcif\", \".ent\"):\n",
    "            if name.endswith(ext):\n",
    "                name = name[: -len(ext)]\n",
    "                break\n",
    "\n",
    "        keys.add(name)\n",
    "    return keys\n",
    "\n",
    "# -------------------------\n",
    "# 3) INTELLIGENT BATCH CREATION\n",
    "# -------------------------\n",
    "print(\"üßπ Organizing local files into batches...\")\n",
    "\n",
    "# Step A: Flatten everything to a temporary \"staging\" area\n",
    "staging_dir = Path(\"/content/temp_staging\")\n",
    "if staging_dir.exists():\n",
    "    shutil.rmtree(staging_dir)\n",
    "staging_dir.mkdir()\n",
    "\n",
    "# Move all existing files from ROOT_QUERY to staging (iterate on a list to avoid traversal issues)\n",
    "file_count = 0\n",
    "for item in list(ROOT_QUERY.rglob(\"*\")):\n",
    "    if item.is_file():\n",
    "        shutil.move(str(item), str(staging_dir / item.name))\n",
    "        file_count += 1\n",
    "\n",
    "# Step B: Create Local Batches (batch_001, batch_002...)\n",
    "all_files = sorted(list(staging_dir.glob(\"*\")))  # sorting ensures consistent batches on retry\n",
    "total_files = len(all_files)\n",
    "num_local_batches = math.ceil(total_files / BATCH_SIZE) if total_files else 0\n",
    "\n",
    "print(f\"   ‚Ä¢ Found {total_files} local files.\")\n",
    "print(f\"   ‚Ä¢ Creating {num_local_batches} local batch(es).\")\n",
    "\n",
    "batch_map = []\n",
    "\n",
    "for i in range(num_local_batches):\n",
    "    batch_name = f\"batch_{i+1:03d}\"\n",
    "    batch_path = ROOT_QUERY / batch_name\n",
    "    batch_path.mkdir(exist_ok=True)\n",
    "\n",
    "    start = i * BATCH_SIZE\n",
    "    end = start + BATCH_SIZE\n",
    "    files_in_batch = all_files[start:end]\n",
    "\n",
    "    for f in files_in_batch:\n",
    "        shutil.move(str(f), str(batch_path / f.name))\n",
    "\n",
    "    batch_map.append({\"name\": batch_name, \"path\": batch_path, \"ids\": \"\"})\n",
    "\n",
    "# Step C: Create Download Batches by CHUNKING the ID list\n",
    "download_ids_list = parse_structure_ids(structure_ids_for_download)\n",
    "\n",
    "# IMPORTANT: Skip IDs that already exist locally (based on filenames we just batched)\n",
    "if download_ids_list:\n",
    "    existing_keys = existing_structure_keys(ROOT_QUERY)  # includes batch_### folders we just created\n",
    "    filtered = [i for i in download_ids_list if i not in existing_keys]\n",
    "    removed = len(download_ids_list) - len(filtered)\n",
    "    if removed > 0:\n",
    "        print(f\"   ‚Ä¢ Skipping {removed} download ID(s) already present as local files.\")\n",
    "    download_ids_list = filtered\n",
    "\n",
    "if download_ids_list:\n",
    "    n_download_batches = math.ceil(len(download_ids_list) / DOWNLOAD_ID_BATCH)\n",
    "\n",
    "    print(f\"   ‚Ä¢ Splitting {len(download_ids_list)} download IDs into {n_download_batches} batch(es) \"\n",
    "          f\"({DOWNLOAD_ID_BATCH} IDs each; last may be smaller).\")\n",
    "\n",
    "    for j in range(n_download_batches):\n",
    "        batch_index = num_local_batches + 1 + j  # continue numbering after local batches\n",
    "\n",
    "        download_batch_name = f\"batch_{batch_index:03d}_downloads_{j+1:03d}\"\n",
    "        download_batch_path = ROOT_QUERY / download_batch_name\n",
    "        download_batch_path.mkdir(exist_ok=True)\n",
    "\n",
    "        start = j * DOWNLOAD_ID_BATCH\n",
    "        end = start + DOWNLOAD_ID_BATCH\n",
    "        ids_chunk = download_ids_list[start:end]\n",
    "        ids_chunk_str = \",\".join(ids_chunk)\n",
    "\n",
    "        batch_map.append({\"name\": download_batch_name, \"path\": download_batch_path, \"ids\": ids_chunk_str})\n",
    "        print(f\"     - Added: {download_batch_name} ({len(ids_chunk)} IDs)\")\n",
    "\n",
    "# Cleanup staging\n",
    "if staging_dir.exists():\n",
    "    shutil.rmtree(staging_dir)\n",
    "\n",
    "# -------------------------\n",
    "# 4) LOG FILE FUNCTIONS\n",
    "# -------------------------\n",
    "def get_completed_batches():\n",
    "    if not LOG_FILE.exists():\n",
    "        return set()\n",
    "    with open(LOG_FILE, \"r\") as f:\n",
    "        return set(line.strip() for line in f.readlines() if line.strip())\n",
    "\n",
    "def mark_batch_complete(b_name):\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(f\"{b_name}\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) EXECUTION LOOP\n",
    "# -------------------------\n",
    "completed_batches = get_completed_batches()\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*40)\n",
    "print(\"üöÄ Starting Execution\")\n",
    "print(f\"   ‚Ä¢ Destination: {DRIVE_DEST}\")\n",
    "print(f\"   ‚Ä¢ Log File: {LOG_FILE.name}\")\n",
    "print(f\"   ‚Ä¢ Previously Completed: {len(completed_batches)}\")\n",
    "print(\"‚ïê\"*40)\n",
    "\n",
    "for batch in batch_map:\n",
    "    b_name = batch[\"name\"]\n",
    "    b_path = batch[\"path\"]\n",
    "    b_ids  = batch[\"ids\"]\n",
    "\n",
    "    # Skip completed batches\n",
    "    if b_name in completed_batches:\n",
    "        print(f\"\\n‚è≠Ô∏è Skipping {b_name} (already completed)\")\n",
    "        # cleanup batch folder to save space\n",
    "        if b_path.exists():\n",
    "            shutil.rmtree(str(b_path))\n",
    "        continue\n",
    "\n",
    "    current_output = OUTPUT_DIR / b_name\n",
    "    current_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n‚ñ∂Ô∏è Processing: {b_name}\")\n",
    "    if b_ids:\n",
    "        print(\"   (Run ZincSight for this chunk...)\")\n",
    "    else:\n",
    "        local_n = len(list(b_path.glob(\"*\"))) if b_path.exists() else 0\n",
    "        print(f\"   (Local files: {local_n})\")\n",
    "    with capture_output() as captured_output:\n",
    "        execute_zincsight(\n",
    "            include_histidine_rotamers,\n",
    "            b_ids,               # \"\" for local; comma-separated IDs for download chunks\n",
    "            str(b_path),         # batch folder (local or download working folder)\n",
    "            str(current_output), # output for this batch\n",
    "            physical_cores,\n",
    "            create_pymol_sessions\n",
    "        )\n",
    "\n",
    "    # Compress and Save\n",
    "    archive_name = shutil.make_archive(str(current_output / b_name), \"gztar\", str(current_output))\n",
    "    final_dest = DRIVE_DEST / f\"ZincSight_{b_name}.tar.gz\"\n",
    "    shutil.copy2(archive_name, final_dest)\n",
    "\n",
    "    print(f\"‚úÖ Completed: {b_name}\")\n",
    "    print(f\"   Saved to: {final_dest.name}\")\n",
    "\n",
    "    # Mark complete\n",
    "    mark_batch_complete(b_name)\n",
    "\n",
    "    # Cleanup\n",
    "    if b_path.exists():\n",
    "        shutil.rmtree(str(b_path))\n",
    "    if current_output.exists():\n",
    "        shutil.rmtree(str(current_output))\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*40)\n",
    "print(\"üèÅ ALL JOBS DONE\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
