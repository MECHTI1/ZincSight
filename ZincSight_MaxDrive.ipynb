{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2227a5adfad709",
   "metadata": {
    "id": "2c2227a5adfad709"
   },
   "source": [
    "# **ZincSight**: Interpretable prediction of zinc ion locations in proteins\n",
    "\n",
    "‚ö†Ô∏è **Important Instructions:**\n",
    "\n",
    "- If ZincSight crashes, please reset the runtime manually:\n",
    "  Go to `Runtime` ‚Üí `Disconnect and delete runtime`, then refresh the page and try again.\n",
    "  <p></p>\n",
    "- If the issue continues, contact **mechtinger1@mail.tau.ac.il** and include the **protein IDs or structures** you used for reproduction and debugging.\n",
    "  <p></p>\n",
    "- For maximum speed, change the hardware accelerator: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí select `TPU`.\n",
    "  <p></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e70ba43144b2857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e70ba43144b2857",
    "outputId": "156427d1-99e0-47e2-d687-77bdf38fb7f5"
   },
   "source": [
    "#@title üß¨ ZincSight: Structure Input & Configuration {display-mode: \"form\"}\n",
    "\n",
    "#@markdown **Enter your protein(s)** in the field below or upload **PDB** or **mmCIF** structures.\n",
    "#@markdown - Query IDs: **PDB** (ASU or biological assembly), **AlphaFoldDB** (UniProt or Model ID), **ESM Metagenomic Atlas**, or **TED Domains**.\n",
    "#@markdown - To upload your own structure files (including `.tar.gz`), check the appropriate box below.\n",
    "#@markdown - **Note:** All input folders will be flattened (all files moved to the root query folder).\n",
    "\n",
    "# --- UI CONTROLS ---\n",
    "\n",
    "#@markdown ### üîë 1. Enter Identifiers\n",
    "structure_ids = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### üìÇ 2. Import Local Files\n",
    "upload_structures = False #@param {type:\"boolean\"}\n",
    "upload_tar_gz = False #@param {type:\"boolean\"}\n",
    "upload_id_list_txt = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ‚òÅÔ∏è 3. Import from Google Drive\n",
    "import_from_google_drive = True # @param {\"type\":\"boolean\"}\n",
    "drive_path = \"\" #@param {type:\"string\"}\n",
    "#@markdown *Path relative to 'My Drive'. Can be a file, folder, or .tar.gz archive.*\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### ‚öôÔ∏è 4. Settings\n",
    "initialize_fresh_query = True #@param {type:\"boolean\"}\n",
    "include_histidine_rotamers = True #@param {type: \"boolean\"}\n",
    "create_pymol_sessions=False #@param {type: \"boolean\"}\n",
    "\n",
    "#@markdown ###  5. output folder name\n",
    "name_of_output_folder = \"\" #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "# --- IMPLEMENTATION ---\n",
    "\n",
    "import re, shutil, tarfile, os\n",
    "from pathlib import Path\n",
    "from google.colab import files, drive\n",
    "\n",
    "QUERY_DIR = Path(\"/content/query_structures\")\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Wipes and recreates the query directory if requested.\"\"\"\n",
    "    if initialize_fresh_query and QUERY_DIR.exists():\n",
    "        print(\"üßπ Initializing query: clearing previous files.\")\n",
    "        shutil.rmtree(QUERY_DIR)\n",
    "    QUERY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def flatten_directory():\n",
    "    \"\"\"Moves all files from subdirectories to the root QUERY_DIR.\"\"\"\n",
    "    # Move files\n",
    "    for p in list(QUERY_DIR.rglob(\"*\")):\n",
    "        if p.is_file() and p.parent != QUERY_DIR:\n",
    "            dest = QUERY_DIR / p.name\n",
    "            # Overwrite if exists, or handle duplicates (here we overwrite for simplicity)\n",
    "            shutil.move(str(p), str(dest))\n",
    "\n",
    "    # Remove empty directories\n",
    "    for p in list(QUERY_DIR.rglob(\"*\")):\n",
    "        if p.is_dir() and p != QUERY_DIR:\n",
    "            try:\n",
    "                p.rmdir() # Only removes empty dirs\n",
    "            except OSError:\n",
    "                pass # Directory not empty (shouldn't happen if logic is correct)\n",
    "\n",
    "def extract_archive(file_path):\n",
    "    \"\"\"Extracts tar.gz and flattens hierarchy.\"\"\"\n",
    "    print(f\"üì¶ Extracting: {file_path.name}...\")\n",
    "    try:\n",
    "        with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=QUERY_DIR)\n",
    "        flatten_directory()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction error: {e}\")\n",
    "\n",
    "def parse_ids(text):\n",
    "    \"\"\"Tokenizes string input into a unique ID list.\"\"\"\n",
    "    return [t for t in re.split(r\"[\\s,]+\", text.strip()) if t]\n",
    "\n",
    "def handle_drive_import(rel_path):\n",
    "    \"\"\"Mounts drive and imports flattened files.\"\"\"\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    src = Path(\"/content/drive/MyDrive\") / rel_path.lstrip('/')\n",
    "\n",
    "    if not src.exists():\n",
    "        print(f\"‚ùå Not found on Drive: {src}\")\n",
    "        return\n",
    "\n",
    "    if src.name.endswith((\".tar.gz\", \".tgz\")):\n",
    "        extract_archive(src)\n",
    "    elif src.is_dir():\n",
    "        print(f\"üìÇ Importing files from Drive folder: {src.name} (Flattening...)\")\n",
    "        # Copy files directly to root, skipping directory structure\n",
    "        count = 0\n",
    "        for f in src.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                shutil.copy2(f, QUERY_DIR)\n",
    "                count += 1\n",
    "        print(f\"   -> Copied {count} files.\")\n",
    "    else:\n",
    "        shutil.copy2(src, QUERY_DIR)\n",
    "        print(f\"üìÑ Imported: {src.name}\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "setup_environment()\n",
    "final_ids = parse_ids(structure_ids)\n",
    "\n",
    "# Local Uploads\n",
    "if upload_id_list_txt:\n",
    "    print(\"üì§ Upload .txt ID list:\")\n",
    "    for name, content in files.upload().items():\n",
    "        final_ids.extend(parse_ids(content.decode(\"utf-8\")))\n",
    "\n",
    "if upload_structures:\n",
    "    print(\"üì§ Upload PDB/mmCIF structures:\")\n",
    "    os.chdir(QUERY_DIR)\n",
    "    uploaded = files.upload()\n",
    "    os.chdir('/content')\n",
    "\n",
    "if upload_tar_gz:\n",
    "    print(\"üì§ Upload .tar.gz archive:\")\n",
    "    for name, content in files.upload().items():\n",
    "        tmp = Path(name)\n",
    "        tmp.write_bytes(content)\n",
    "        extract_archive(tmp)\n",
    "        tmp.unlink()\n",
    "\n",
    "# Drive Import\n",
    "if import_from_google_drive and drive_path:\n",
    "    handle_drive_import(drive_path)\n",
    "\n",
    "# Force flatten one last time to be safe\n",
    "flatten_directory()\n",
    "\n",
    "# --- RESULTS SUMMARY ---\n",
    "\n",
    "unique_ids = list(dict.fromkeys(final_ids))\n",
    "structure_ids_for_download = \",\".join(unique_ids)\n",
    "\n",
    "# Identify all structural files (shallow scan is sufficient now)\n",
    "valid_exts = {'.pdb', '.cif', '.ent', '.gz'}\n",
    "found_files = [f for f in QUERY_DIR.glob(\"*\") if f.is_file() and f.suffix.lower() in valid_exts]\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*50)\n",
    "print(f\"üöÄ **ZincSight: Input Ready (FLATTENED)**\")\n",
    "print(f\"‚Ä¢ Identifiers to fetch: {len(unique_ids)}\")\n",
    "print(f\"‚Ä¢ Local files detected: {len(found_files)}\")\n",
    "print(f\"‚Ä¢ Folder structure: All files moved to root ‚úÖ\")\n",
    "\n",
    "if found_files:\n",
    "     # Quick check for duplicates or overwrites\n",
    "     print(f\"‚Ä¢ Sample file: {found_files[0].name}\")\n",
    "\n",
    "if not unique_ids and not found_files:\n",
    "    print(\"\\n‚ö†Ô∏è **Warning:** No data found. Check your inputs.\")\n",
    "print(\"‚îÄ\"*50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad67a174c4150f7",
   "metadata": {
    "collapsed": true,
    "id": "8ad67a174c4150f7"
   },
   "outputs": [],
   "source": [
    "#@title Execute ZincSight (Auto-Batching 50k + Resume Log) {display-mode: \"form\"}\n",
    "from IPython.utils.capture import capture_output\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import math\n",
    "import multiprocessing\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "# --- 1. SETUP & DEPENDENCIES ---\n",
    "SETUP_MARKER = \"/content/ENV_SETUP.marker\"\n",
    "if not os.path.exists(SETUP_MARKER):\n",
    "    print(\"üîß Installing dependencies (this takes ~2 mins)...\")\n",
    "    with capture_output():\n",
    "        if not os.path.exists(\"/content/ZincSight\"):\n",
    "            !git clone https://github.com/MECHTI1/ZincSight.git\n",
    "        %cd /content/ZincSight\n",
    "        !pip install -r requirements.txt\n",
    "        sys.path.append(\"/content/ZincSight\")\n",
    "    open(SETUP_MARKER, \"w\").close()\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "sys.path.insert(0, \"/content/ZincSight\")\n",
    "from main_execute import execute_zincsight\n",
    "\n",
    "# --- 2. CONFIGURATION ---\n",
    "ROOT_QUERY = Path(\"/content/query_structures\")\n",
    "OUTPUT_DIR = Path(\"/content/output\")\n",
    "DRIVE_DEST = Path(f\"/content/drive/MyDrive/{name_of_output_folder}_ZincSight_results\")\n",
    "LOG_FILE = DRIVE_DEST / \"zincsight_batch_log.txt\"\n",
    "BATCH_SIZE = 50000  # <--- MAX FILES PER BATCH\n",
    "\n",
    "ROOT_QUERY.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DRIVE_DEST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hardware check\n",
    "physical_cores = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "# --- 3. INTELLIGENT BATCH CREATION ---\n",
    "\n",
    "print(\"üßπ Organizing files into batches of 50,000...\")\n",
    "\n",
    "# Step A: Flatten everything to a temporary \"staging\" area\n",
    "staging_dir = Path(\"/content/temp_staging\")\n",
    "if staging_dir.exists(): shutil.rmtree(staging_dir)\n",
    "staging_dir.mkdir()\n",
    "\n",
    "# Move all existing files/folders from ROOT_QUERY to staging\n",
    "file_count = 0\n",
    "for item in ROOT_QUERY.rglob(\"*\"):\n",
    "    if item.is_file():\n",
    "        shutil.move(str(item), str(staging_dir / item.name))\n",
    "        file_count += 1\n",
    "\n",
    "# Step B: Create Batches (batch_001, batch_002...)\n",
    "all_files = sorted(list(staging_dir.glob(\"*\"))) # Sorting ensures consistent batches on retry\n",
    "total_files = len(all_files)\n",
    "num_batches = math.ceil(total_files / BATCH_SIZE)\n",
    "\n",
    "if num_batches == 0 and structure_ids_for_download.strip():\n",
    "    num_batches = 1\n",
    "\n",
    "print(f\"   ‚Ä¢ Found {total_files} local files.\")\n",
    "print(f\"   ‚Ä¢ Creating {num_batches} batch(es).\")\n",
    "\n",
    "# Distribute files into batch folders\n",
    "batch_map = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch_name = f\"batch_{i+1:03d}\"\n",
    "    batch_path = ROOT_QUERY / batch_name\n",
    "    batch_path.mkdir(exist_ok=True)\n",
    "\n",
    "    start = i * BATCH_SIZE\n",
    "    end = start + BATCH_SIZE\n",
    "    files_in_batch = all_files[start:end]\n",
    "\n",
    "    for f in files_in_batch:\n",
    "        shutil.move(str(f), str(batch_path / f.name))\n",
    "\n",
    "    batch_map.append({\n",
    "        \"name\": batch_name,\n",
    "        \"path\": batch_path,\n",
    "        \"ids\": \"\"\n",
    "    })\n",
    "\n",
    "# Step C: Handle Downloads (The \"Download Batch\")\n",
    "if structure_ids_for_download and structure_ids_for_download.strip():\n",
    "    download_batch_name = f\"batch_{num_batches + 1:03d}_downloads\"\n",
    "    download_batch_path = ROOT_QUERY / download_batch_name\n",
    "    download_batch_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch_map.append({\n",
    "        \"name\": download_batch_name,\n",
    "        \"path\": download_batch_path,\n",
    "        \"ids\": structure_ids_for_download\n",
    "    })\n",
    "    print(f\"   ‚Ä¢ Added dedicated batch for downloads: {download_batch_name}\")\n",
    "\n",
    "# Cleanup Staging\n",
    "if staging_dir.exists(): shutil.rmtree(staging_dir)\n",
    "\n",
    "# --- 4. LOG FILE FUNCTIONS ---\n",
    "\n",
    "def get_completed_batches():\n",
    "    if not LOG_FILE.exists():\n",
    "        return set()\n",
    "    with open(LOG_FILE, \"r\") as f:\n",
    "        return set(line.strip() for line in f.readlines())\n",
    "\n",
    "def mark_batch_complete(b_name):\n",
    "    with open(LOG_FILE, \"a\") as f:\n",
    "        f.write(f\"{b_name}\\n\")\n",
    "\n",
    "# --- 5. EXECUTION LOOP ---\n",
    "\n",
    "completed_batches = get_completed_batches()\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*40)\n",
    "print(f\"üöÄ **Starting Execution**\")\n",
    "print(f\"   ‚Ä¢ Destination: {DRIVE_DEST}\")\n",
    "print(f\"   ‚Ä¢ Log File: {LOG_FILE.name}\")\n",
    "print(f\"   ‚Ä¢ Previously Completed: {len(completed_batches)}\")\n",
    "print(\"‚ïê\"*40)\n",
    "\n",
    "for batch in batch_map:\n",
    "    b_name = batch['name']\n",
    "    b_path = batch['path']\n",
    "    b_ids  = batch['ids']\n",
    "\n",
    "    # --- CHECK LOG FILE ---\n",
    "    if b_name in completed_batches:\n",
    "        print(f\"\\n‚è≠Ô∏è **Skipping {b_name}** (Already marked complete in log)\")\n",
    "        # Clean up the folder since we don't need it\n",
    "        if b_path.exists(): shutil.rmtree(str(b_path))\n",
    "        continue\n",
    "    # ----------------------\n",
    "\n",
    "    # Output folder for this specific batch\n",
    "    current_output = OUTPUT_DIR / b_name\n",
    "    current_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n‚ñ∂Ô∏è **Processing: {b_name}**\")\n",
    "    if b_ids: print(f\"   (Downloading IDs...)\")\n",
    "    else: print(f\"   (Local files: {len(list(b_path.glob('*')))})\")\n",
    "\n",
    "    try:\n",
    "        execute_zincsight(\n",
    "            include_histidine_rotamers,\n",
    "            b_ids,\n",
    "            str(b_path),\n",
    "            str(current_output),\n",
    "            physical_cores,\n",
    "            create_pymol_sessions\n",
    "        )\n",
    "\n",
    "        # Compress and Save\n",
    "        archive_name = shutil.make_archive(str(current_output / b_name), 'gztar', str(current_output))\n",
    "        final_dest = DRIVE_DEST / f\"ZincSight_{b_name}.tar.gz\"\n",
    "        shutil.copy2(archive_name, final_dest)\n",
    "\n",
    "        print(f\"‚úÖ **Completed:** {b_name}\")\n",
    "        print(f\"   Saved to: {final_dest.name}\")\n",
    "\n",
    "        # --- UPDATE LOG ---\n",
    "        mark_batch_complete(b_name)\n",
    "        # ------------------\n",
    "\n",
    "        # Cleanup\n",
    "        shutil.rmtree(str(b_path))\n",
    "        shutil.rmtree(str(current_output))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå **Failed:** {b_name}\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        # We do NOT mark as complete, so it runs again next time\n",
    "\n",
    "print(\"\\n\" + \"‚ïê\"*40)\n",
    "print(\"üèÅ **ALL JOBS DONE**\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
